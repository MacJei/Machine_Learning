{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "week12_cnn_practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mcQFvgvQ9PTF"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1YCiRQPc9PR3"
      },
      "source": [
        "## 12: Convolutional neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qc7OKSJUQAUx"
      },
      "source": [
        "Inspiration for this notebook is taken from YSDA materials\n",
        "\n",
        "__Colab is highly recommended to work with this notebook__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Yav9Boc9PR6"
      },
      "source": [
        "### About CNNs\n",
        "Convolutional layers extract features - quantitative representations of some attributes. \n",
        "\n",
        "After the extraction you can use these features for classification, for example.\n",
        "\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/act.png?raw=1\" width=\"800\">\n",
        "\n",
        "#### Convolution:\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/convolution.gif?raw=1\" width=\"400\">\n",
        "\n",
        "### Pooling:\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/pooling.gif?raw=1\" width=\"400\">\n",
        "\n",
        "\n",
        "## Deeper layer $\\to$ more complex features.\n",
        "\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/feat.png?raw=1\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q4SmvtDxQAUz"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6NJ5nCbsQAU0"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KGRC0NKUQAU2"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YPHzePLvQAU2"
      },
      "source": [
        "# Task: Cats vs. Dogs Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yo-f7ZO-QAU3"
      },
      "source": [
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/cat_dog_sota.jpg?raw=1\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's try to build a small convolutional neural network capable of separating cat images from dog images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AtU27J1B9PR7"
      },
      "source": [
        "## Datasets in pyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOHpVkrs9PR9"
      },
      "source": [
        "Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can convert this array into a torch.*Tensor.\n",
        "\n",
        "- For images, packages such as *Pillow*, *OpenCV* are useful\n",
        "- For audio, packages such as *scipy* and *librosa*\n",
        "- For text, either raw *Python* or *Cython* based loading, or *NLTK* and *SpaCy* are useful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CbVbil_O9PR-"
      },
      "source": [
        "We are dealing with images, so let's have a look at image data loading in pyTorch for [Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats) classification competition.\n",
        "\n",
        "The link for data downloading is in the cell below.\n",
        "\n",
        "Training set size is reduced for performace. If you have enough computational resources, use [this link](https://www.dropbox.com/s/h2vhfxb0j3eazu1/train.zip) for downloading instead of the latter one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeacfNQVtgGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training set with 11K images\n",
        "! wget https://www.dropbox.com/s/gqdo90vhli893e0/data.zip\n",
        "! unzip -qq data.zip -d data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw4aN8OktgHG",
        "colab_type": "text"
      },
      "source": [
        "Now let's look at the way datasets are processed in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcc8ahZF9PR_",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import os\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZV2hezd9PSD"
      },
      "source": [
        "Images should be stored class-wise in PC memory: each image class have to be represented as a subfolder with the corresponding image data. `ImageFolder` takes the path to 'root' directory of such structure, e.g. DATA_PATH:\n",
        "\n",
        "- DATA_PATH/dog/xxx.png\n",
        "- DATA_PATH/dog/xxy.png\n",
        "- DATA_PATH/dog/xxz.png\n",
        "- DATA_PATH/cat/123.png\n",
        "- DATA_PATH/cat/nsdf3.png\n",
        "- DATA_PATH/cat/asd932_.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fpb7x6ZL9PSD"
      },
      "source": [
        "Dataset images are of different size.\n",
        "\n",
        "Batch generator expects a batch of tensors of the same dimensions, thus we need to rescale images in the dataset during data loading.\n",
        "\n",
        "Let's see at the image size distributions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmBH5YMt9PSJ",
        "colab": {}
      },
      "source": [
        "### Let's have a cell with global hyperparameters for the CNNs in this notebook\n",
        "\n",
        "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
        "DATA_PATH = r\"data\" # PATH TO THE DATASET\n",
        "\n",
        "# Number of threads for data loader\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# Image size: even though image sizes are bigger than 64, we use this to speed up training\n",
        "SIZE_H = SIZE_W = 96\n",
        "\n",
        "# Number of classes in the dataset\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
        "EPOCH_NUM = 30\n",
        "\n",
        "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Images mean and std channelwise\n",
        "image_mean = [0.485, 0.456, 0.406]\n",
        "image_std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Last layer (embeddings) size for CNN models\n",
        "EMBEDDING_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3BhPGl6K9PSM"
      },
      "source": [
        "Let's define a transformer to be used as image preprocessing step prior to creating pyTorch image dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RAvQCX6h9PSN",
        "colab": {}
      },
      "source": [
        "transformer = transforms.Compose([\n",
        "    transforms.Resize((SIZE_H, SIZE_W)),        # scaling images to fixed size\n",
        "    transforms.ToTensor(),                      # converting to tensors\n",
        "    transforms.Normalize(image_mean, image_std) # normalize image data per-channel\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pCNoQ0ioQAVN"
      },
      "source": [
        "Create an ImageFolder instance to be used during training, validation and testing phases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U7T0jy0p9PSP",
        "colab": {}
      },
      "source": [
        "# load dataset using torchvision.datasets.ImageFolder\n",
        "train_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'train_11k'), transform=transformer)\n",
        "val_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'val'), transform=transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tgm8E3ou9PSR",
        "colab": {}
      },
      "source": [
        "# load test data also, to be used for final evaluation\n",
        "test_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'test_labeled'), transform=transformer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "np5Qwd9uQAVT"
      },
      "source": [
        "Save sample num for further use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C8Bl3zkP9PSU",
        "colab": {}
      },
      "source": [
        "n_train, n_val, n_test = len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wEhvhn2gQAVW"
      },
      "source": [
        "Now let's create a DataLoader instance, which uses ImageFolder instance to generate batches of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3fRONyLZ9PSX",
        "colab": {}
      },
      "source": [
        "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=NUM_WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0hoG_A8w9PSZ",
        "colab": {}
      },
      "source": [
        "val_batch_gen = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            num_workers=NUM_WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WPMTJs-pAezL",
        "colab": {}
      },
      "source": [
        "test_batch_gen = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             num_workers=NUM_WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OPrOPuZwQAVe"
      },
      "source": [
        "Let's create a helper function to vizualize images from our data loaders (and also make sure data was properly loaded)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFkq7psDQAVe",
        "colab": {}
      },
      "source": [
        "def plot_from_batch_generator(batch_gen):\n",
        "    data_batch, label_batch = next(iter(batch_gen))\n",
        "    grid_size = (3, 3)\n",
        "    f, axarr = plt.subplots(*grid_size)\n",
        "    f.set_size_inches(15,10)\n",
        "    class_names = batch_gen.dataset.classes\n",
        "    for i in range(grid_size[0] * grid_size[1]):\n",
        "        \n",
        "        # read images from batch to numpy.ndarray and change axes order [H, W, C] -> [H, W, C]\n",
        "        batch_image_ndarray = np.transpose(data_batch[i].numpy(), [1, 2, 0])\n",
        "        \n",
        "        # inverse normalization for image data values back to [0,1] and clipping the values for correct pyplot.imshow()\n",
        "        src = np.clip(image_std * batch_image_ndarray + image_mean, 0, 1)\n",
        "        \n",
        "        # display batch samples with labels\n",
        "        sample_title = 'Label = %d (%s)' % (label_batch[i], class_names[label_batch[i]])\n",
        "        axarr[i // grid_size[0], i % grid_size[0]].imshow(src)\n",
        "        axarr[i // grid_size[0], i % grid_size[0]].set_title(sample_title)\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "utWRZc8d9PSb",
        "colab": {}
      },
      "source": [
        "plot_from_batch_generator(train_batch_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ElonL_B29PSe"
      },
      "source": [
        "# Building a network\n",
        "\n",
        "Simple neural networks with layers applied on top of one another can be implemented as `torch.nn.Sequential` - just add a list of pre-built modules and let it train.\n",
        "\n",
        "Let's use them to create a baseline for our models.\n",
        "\n",
        "The following cell implements helper CNN layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5cUll-S19PSg",
        "colab": {}
      },
      "source": [
        "# a special module that converts [batch, channel, w, h] to [batch, units]: tf/keras style\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        # finally we have it in pytorch\n",
        "        return torch.flatten(x, start_dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XwXUGWGJQAVu"
      },
      "source": [
        "## Task 0: Multi-layer fully-connected network\n",
        "\n",
        "Look at the NN structure proposed below.\n",
        "\n",
        "We will use this model as a baseline for classification task.\n",
        "\n",
        "As you already know, fully-connetcted networks are not translation invariant and perform worse on image data, so resulting accuracy will be lower than for convolutional neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EUZan5MN9PSh",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential()\n",
        "\n",
        "# reshape from \"images\" to flat vectors\n",
        "model.add_module('flatten', Flatten())\n",
        "\n",
        "# dense \"head\"\n",
        "model.add_module('dense1', nn.Linear(3 * SIZE_H * SIZE_W, 256))\n",
        "model.add_module('dense1_relu', nn.ReLU())\n",
        "model.add_module('dropout1', nn.Dropout(0.1))\n",
        "model.add_module('dense3', nn.Linear(256, EMBEDDING_SIZE))\n",
        "model.add_module('dense3_relu', nn.ReLU())\n",
        "model.add_module('dropout3', nn.Dropout(0.1))\n",
        "# logits for NUM_CLASSES=2: cats and dogs\n",
        "model.add_module('dense4_logits', nn.Linear(EMBEDDING_SIZE, NUM_CLASSES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZvXU4V5DQAVx"
      },
      "source": [
        "Print model summary for sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xCPIGC_SQAVy",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "summary(model, (3, SIZE_H, SIZE_W), device='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZcVtn-tgMI",
        "colab_type": "text"
      },
      "source": [
        "## Construct training pipeline\n",
        "\n",
        "Now that we can properly load the data and define the NN model structure in terms of input/output, we need to build functions, that will perform forward and backward passes for the model so the backpropagation will optimize the NN weights in accordance with used loss function.\n",
        "\n",
        "**Question**: What loss funtion do we need to use for the classification task?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XVpuVaD69PSk",
        "colab": {}
      },
      "source": [
        "loss_function = # <YOUR CODE HERE>\n",
        "\n",
        "def compute_loss(model, data_batch):\n",
        "    \"\"\" Compute the loss using loss_function for the batch of data and return mean loss value for this batch.\"\"\"\n",
        "    # load the data\n",
        "    img_batch = data_batch['img']\n",
        "    label_batch = data_batch['label']\n",
        "    \n",
        "    # forward pass\n",
        "    logits = model(img_batch)\n",
        "    \n",
        "    # loss computation\n",
        "    loss = loss_function(logits, label_batch)\n",
        "    \n",
        "    return loss, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ-3Ooi8tgMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test your loss function calculation\n",
        "batch_size_test = 8\n",
        "img = np.random.randint(0, 255, size=(batch_size_test,3, SIZE_H, SIZE_W)).astype(np.float32) / 255\n",
        "img = (img - np.array(image_mean).reshape(1, 3, 1, 1)) / np.array(image_std).reshape(1,3,1,1)\n",
        "img_tensor = torch.Tensor(img)\n",
        "label = torch.Tensor(np.random.randint(0,2, size=batch_size_test)).type(torch.long)\n",
        "data_batch = {'img': img_tensor, 'label': label}\n",
        "loss, model = compute_loss(model, data_batch)\n",
        "loss_np = loss.detach().cpu().numpy()\n",
        "\n",
        "assert loss_np.size == 1, 'compute_loss() shall return a single value of a loss averaged over a batch of samples'\n",
        "\n",
        "print('Loss value: {}'.format(loss.detach().cpu().numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nou9meOgtgMd",
        "colab_type": "text"
      },
      "source": [
        "We will also need some training progress monitoring. We would like to calculate classification metrics on the validation set each epoch and plot score distributions for the two classes we have.\n",
        "\n",
        "All the necessary stuff is implemented in the following cell. If you wish to add more binary classification metrics, feel free to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFS4IaRatgMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def accuracy(scores, labels, threshold=0.5):\n",
        "    assert type(scores) is np.ndarray and type(labels) is np.ndarray\n",
        "    predicted = np.array(scores > threshold).astype(np.int32)\n",
        "    return np.mean(predicted == labels)\n",
        "\n",
        "def f1(scores, labels, threshold=0.5):\n",
        "    assert type(scores) is np.ndarray and type(labels) is np.ndarray\n",
        "    predicted = np.array(scores > threshold).astype(np.int32)\n",
        "    return f1_score(labels, predicted)\n",
        "\n",
        "# you may add other metrics if you wish\n",
        "tracked_metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'f1-score': f1\n",
        "}\n",
        "\n",
        "def calculate_metrics(scores, labels, print_log=False):\n",
        "    \"\"\" Compute all the metrics from tracked_metrics dict using scores and labels.\"\"\"\n",
        "    \n",
        "    assert len(labels) == len(scores), print('Label and score lists are of different size')\n",
        "    \n",
        "    scores_array = np.array(scores).astype(np.float32)\n",
        "    labels_array = np.array(labels)\n",
        "    \n",
        "    metric_results = {}\n",
        "    for k,v in tracked_metrics.items():\n",
        "        metric_value = v(scores_array, labels_array)\n",
        "        metric_results[k] = metric_value\n",
        "    \n",
        "    if print_log:\n",
        "        print(' | '.join(['{}: {:.4f}'.format(k,v) for k, v in metric_results.items()]))\n",
        "    \n",
        "    return metric_results\n",
        "\n",
        "\n",
        "def get_score_distributions(epoch_result_dict):\n",
        "    \"\"\" Return per-class score arrays.\"\"\"\n",
        "    scores = epoch_result_dict['scores']\n",
        "    labels = epoch_result_dict['labels']\n",
        "    \n",
        "    # save per-class scores\n",
        "    for class_id in [0, 1]:\n",
        "        epoch_result_dict['scores_' + str(class_id)] = np.array(scores)[np.array(labels) == class_id]\n",
        "    \n",
        "    return epoch_result_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ixp5qMwFQAV6"
      },
      "source": [
        "The next cell implements the model evaluation part, which only calculates the scores in the forward pass given the batch generator. Optionally, it also computes the classification metrics and logs them.\n",
        "\n",
        "**Warning:** Please note, that we will use 'dog class probability' as the CNN score to be used for score distribution plots and classification metrics, as the label of 'dog class' is '1'. In other words, here we will use output neuron 1 and not use output neuron 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3q2W23jiQAV7",
        "colab": {}
      },
      "source": [
        "@torch.no_grad() # we do not need to save gradients on evaluation\n",
        "def test_model(model, batch_generator, subset_name='test', print_log=True, plot_scores=False):\n",
        "    \"\"\" Evaluate the model using data from batch_generator and metrics defined above.\"\"\"\n",
        "    \n",
        "    # disable dropout / use averages for batch_norm\n",
        "    model.train(False)\n",
        "    \n",
        "    # save scores, labels and loss values for performance logging\n",
        "    score_list = []\n",
        "    label_list = []\n",
        "    loss_list = []\n",
        "    \n",
        "    for X_batch, y_batch in batch_generator:\n",
        "        \n",
        "        # do the forward pass\n",
        "        logits = model(X_batch.to(device))\n",
        "        scores = # <YOUR CODE HERE>\n",
        "        labels = y_batch.numpy().tolist()\n",
        "        \n",
        "        # compute loss value\n",
        "        loss = loss_function(logits, y_batch.to(device))\n",
        "        \n",
        "        # save the necessary data\n",
        "        loss_list.append(loss.detach().cpu().numpy().tolist())\n",
        "        score_list.extend(scores)\n",
        "        label_list.extend(labels)\n",
        "    \n",
        "    if print_log:\n",
        "        print(\"Results on {} set | \".format(subset_name), end='')\n",
        "    \n",
        "    metric_results = calculate_metrics(score_list, label_list, print_log)\n",
        "    metric_results['scores'] = score_list\n",
        "    metric_results['labels'] = label_list\n",
        "    metric_results['loss'] = loss_list\n",
        "    \n",
        "    return metric_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFC3MsP8tgMx",
        "colab_type": "text"
      },
      "source": [
        "This cell implements the model training part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ciJe2vO3tgM1",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def train_model(model, train_batch_generator, val_batch_generator, opt, ckpt_name=None, n_epochs=EPOCH_NUM, visualize=True):\n",
        "    \"\"\"\n",
        "    Run training: forward/backward pass using train_batch_generator and evaluation using val_batch_generator.\n",
        "    Log performance using loss monitoring and score distribution plots for validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    train_loss, val_loss = [], [1]\n",
        "    val_loss_idx = [0]\n",
        "    best_model = None\n",
        "    top_val_accuracy = 0\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Train phase\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        for (X_batch, y_batch) in tqdm(train_batch_generator, desc='Training', leave=False):\n",
        "            \n",
        "            # move data to target device\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            data_batch = {'img': X_batch, 'label': y_batch}\n",
        "            \n",
        "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
        "            loss, model = compute_loss(model, data_batch)\n",
        "            \n",
        "            # compute backward pass\n",
        "            # <YOUR CODE HERE>\n",
        "            \n",
        "            # log train loss\n",
        "            train_loss.append(loss.detach().cpu().numpy())\n",
        "        \n",
        "        # Evaluation phase\n",
        "        metric_results = test_model(model, val_batch_generator, subset_name='val')\n",
        "        metric_results = get_score_distributions(metric_results)\n",
        "        \n",
        "        if visualize:\n",
        "            clear_output()\n",
        "        \n",
        "        # Logging\n",
        "        val_loss_value = np.mean(metric_results['loss'])\n",
        "        val_loss_idx.append(len(train_loss))\n",
        "        val_loss.append(val_loss_value)\n",
        "        \n",
        "        \n",
        "        if visualize:\n",
        "            # tensorboard for the poor\n",
        "            fig = plt.figure(figsize=(15,5))\n",
        "            ax1 = fig.add_subplot(121)\n",
        "            ax2 = fig.add_subplot(122)\n",
        "            \n",
        "            ax1.plot(train_loss, color='b', label='train')\n",
        "            ax1.plot(val_loss_idx, val_loss, color='c', label='val')\n",
        "            ax1.legend()\n",
        "            ax1.set_title('Train/val loss.')\n",
        "            \n",
        "            ax2.hist(metric_results['scores_0'], bins=50, range=[0,1.01], color='r', alpha=0.7, label='cats')\n",
        "            ax2.hist(metric_results['scores_1'], bins=50, range=[0,1.01], color='g', alpha=0.7, label='dogs')\n",
        "            ax2.legend()\n",
        "            ax2.set_title('Validation set score distribution.')\n",
        "            \n",
        "            plt.show()\n",
        "        \n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, n_epochs, time.time() - start_time))\n",
        "        train_loss_value = np.mean(train_loss[-n_train // BATCH_SIZE :])\n",
        "        val_accuracy_value = metric_results['accuracy']\n",
        "        if val_accuracy_value > top_val_accuracy and ckpt_name is not None:\n",
        "            top_val_accuracy = val_accuracy_value\n",
        "            \n",
        "            # save checkpoint of the best model to disk\n",
        "            with open(ckpt_name, 'wb') as f: torch.save(model, f)\n",
        "        \n",
        "    return model, opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TuOcy55a9PSp"
      },
      "source": [
        "## Training on minibatches\n",
        "\n",
        "* We got 11k images (22k for full train set), that's way too many for a full-batch SGD. Let's train on minibatches instead\n",
        "* For visualization purposes we propose to plot train/val loss graphs and validation score distribution for CNN predictions over images of cats (class_0) and dogs (class_1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kF0YxUDU9PSq",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "opt.zero_grad()\n",
        "ckpt_name = 'model_base.ckpt'\n",
        "model = model.to(device)\n",
        "model, opt = train_model(model, train_batch_gen, val_batch_gen, opt,ckpt_name=ckpt_name, n_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kUlkQqfn9PSu"
      },
      "source": [
        "Don't wait for too many epochs. You can interrupt training after 5-15 epochs once validation accuracy stops going up.\n",
        "\n",
        "**Question:** You might have noticed that training code plots per-class score distribution. How can we use this plot to analyze model performance on binary classification task?\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "## Evaluate the best model using test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pcAqRh-0dtC_",
        "colab": {}
      },
      "source": [
        "best_model = None\n",
        "with open(ckpt_name, 'rb') as f:\n",
        "    best_model = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model, test_batch_gen, 'test')\n",
        "\n",
        "if val_stats['f1_score'] > 0.75 and test_stats['f1_score'] > 0.75:\n",
        "    print('You have made fully-connected NN perform surprisingly well, call for the assistant.')\n",
        "else:\n",
        "    print('Well done, move on to next block to improve performance.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q4QBl_VG9PS0"
      },
      "source": [
        "# Task I: small convolution net\n",
        "## First step\n",
        "\n",
        "**conv-pool-conv-pool-dense-dense-everybody!**\n",
        "\n",
        "Let's create a mini-convolutional network with roughly such architecture:\n",
        "* Input layer\n",
        "* 4 classic convolutional blocks `convolution->relu->pool`: \n",
        "  * 3x3 convolution with 32 -> 32 -> 64 -> 128 filters and _ReLU_ activation\n",
        "  * 2x2 pooling (or set previous convolution stride to 3)\n",
        " * Flatten\n",
        "* 30% Dropout \n",
        "* Dense layer with 128 neurons and _ReLU_ activation\n",
        "* 30% dropout\n",
        "* Output dense layer.\n",
        "\n",
        "__Convolutional layers__ in torch are just like all other layers, but with a specific set of parameters:\n",
        "\n",
        "__`...`__\n",
        "\n",
        "__`model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3)) # convolution`__\n",
        "\n",
        "__`model.add_module('pool1', nn.MaxPool2d(2)) # max pooling 2x2`__\n",
        "\n",
        "__`...`__\n",
        "\n",
        "\n",
        "Once you're done (and compute_loss no longer raises errors), train it with __Adam__ optimizer with learning_rate=3e-4 (Karpathy Constant)\n",
        "\n",
        "If everything is right, you should get at least __75%__ validation accuracy.\n",
        "\n",
        "__HACK_OF_THE_DAY__ :the number of channels must be in the order of the number of class_labels\n",
        "\n",
        "__HACK_OF_THE_DAY_2__ : you may set stride=2 for Conv2d layers to increase learning speed, but keep in mind tensor shapes\n",
        "\n",
        "__HACK_OF_THE_DAY_3__  : it might be useful to use 'VGG-like' structure as a baseline for this task: \n",
        "    * every CNN layer with 2x2 maxpooling / stride=2 should be followed by increasing the number of output channels x2\n",
        "    * before the fully-connected layer the tensor H and W should be relatively small (less than 10)\n",
        "    * in other words, the less H and W of tensor are, the more should you increase C in order to keep more information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jvLGLn3n9PS1",
        "colab": {}
      },
      "source": [
        "model_cnn = nn.Sequential()\n",
        "\n",
        "# Your code here: CONV->POOL->CONV-POOL->... as many as you wish\n",
        "# End of your code here\n",
        "\n",
        "# global max pooling\n",
        "model_cnn.add_module('global_max_pooling', nn.AdaptiveMaxPool2d(1))\n",
        "# dropout for regularization\n",
        "model_cnn.add_module('dropout', nn.Dropout(0.3))\n",
        "# \"flatten\" the data\n",
        "model_cnn.add_module('flat', Flatten())\n",
        "# last fully-connected layer, used to create embedding vectors\n",
        "model_cnn.add_module('fc', nn.Linear(128, EMBEDDING_SIZE))\n",
        "model_cnn.add_module('relu', nn.ReLU())\n",
        "\n",
        "model_cnn.add_module('dropout_6', nn.Dropout(0.3))\n",
        "\n",
        "# logits for NUM_CLASSES=2 classes\n",
        "model_cnn.add_module('fc_logits', nn.Linear(EMBEDDING_SIZE, NUM_CLASSES, bias=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "abI_sZp1QAWJ"
      },
      "source": [
        "\n",
        "__Hint:__ If you don't want to compute shapes by hand, just plug in any shape (e.g. 1 unit) and run compute_loss. You will see something like this:\n",
        "\n",
        "__`RuntimeError: size mismatch, m1: [5 x 1960], m2: [1 x 64] at /some/long/path/to/torch/operation`__\n",
        "\n",
        "See the __1960__ there? That's your actual input shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7aK-IzAKCcLi"
      },
      "source": [
        "Let's see the basic structure of our model and at the same time perform a sanity check for tensor dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G-MTzSnZ9PS7",
        "colab": {}
      },
      "source": [
        "summary(model_cnn, (3, SIZE_H, SIZE_W), device='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C69Gtezv9PS9"
      },
      "source": [
        "## Training\n",
        "\n",
        "We may use the same training pipeline, that we defined above, as it does not depend on model structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nn3tZF8e9PS9",
        "colab": {}
      },
      "source": [
        "model_cnn = model_cnn.to(device)\n",
        "opt = torch.optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
        "opt.zero_grad()\n",
        "ckpt_name_cnn='model_cnn.ckpt'\n",
        "model_cnn, opt = train_model(model_cnn, train_batch_gen, val_batch_gen,opt, ckpt_name=ckpt_name_cnn, n_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6GSpqx2KrQfM"
      },
      "source": [
        "**A kind reminder again:** don't wait for too many  epochs. You can interrupt training after 5-20 epochs once validation accuracy stops going up.\n",
        "```\n",
        "```\n",
        "## Evaluation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jnXSYJUIQAWT",
        "colab": {}
      },
      "source": [
        "best_model_cnn = None\n",
        "with open(ckpt_name_cnn, 'rb') as f:\n",
        "    best_model_cnn = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model_cnn, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model_cnn, test_batch_gen, 'test')\n",
        "\n",
        "if val_stats['f1_score'] > 0.75 and test_stats['f1_score'] > 0.75:\n",
        "    print('You have achieved the baseline for this task.')\n",
        "else:\n",
        "    print('Train for some more time.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VY3-lvGSq_Sd",
        "colab": {}
      },
      "source": [
        "# Task 2: Fine-tuning\n",
        "\n",
        "## In practice it is easier to use pre-trained NN\n",
        "\n",
        "<img src=\"img/ft.jpg\" width=\"600\">\n",
        "\n",
        "We may see, that our current model performs quite well even after a small number of training epochs.\n",
        "\n",
        "But for more complicated image classification or other computer vision tasks, it may be difficult to train CNN model from scratch.\n",
        "\n",
        "State-of-the-art models consist of huge number of layers (100-200 convolutional blocks) and require powerful hardware to converge.\n",
        "\n",
        "Thankfully, there are lots of pre-trained models available to be used for your own task, only slightly changing some of the final layers to your data.\n",
        "\n",
        "This is called fine-tuning.\n",
        "\n",
        "Let's try to load a pre-trained [ResNet-18](https://arxiv.org/abs/1512.03385) model from torch archive and fine-tune its final layers.\n",
        "\n",
        "### ResNet (Shortcut + Batch Normalization)\n",
        " \n",
        "<img src=\"img/resnet.png\" width=\"800\">"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wznobt_oQAW_",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model\n",
        "model_resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Disable gradient updates for all the layers except  the final layer\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_resnet18.fc.in_features\n",
        "model_resnet18.fc = #YOUR CODE HERE: add your own prediction part: FC layer for 2 classes\n",
        "\n",
        "# Use available device for calculations\n",
        "model_resnet18 = model_resnet18.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rDvxPBkQAXC",
        "colab": {}
      },
      "source": [
        "summary(model_resnet18, (3, SIZE_H, SIZE_W))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N9u_c-t_QAXF"
      },
      "source": [
        "## Training (only for final layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74zaJbG6q_PN",
        "colab": {}
      },
      "source": [
        "# Observe that only parameters of final layer are being optimized as opposed to before\n",
        "opt_resnet = torch.optim.Adam(model_resnet18.fc.parameters(), lr=1e-3)\n",
        "ckpt_name_resnet18='model_resnet_18_finetune.ckpt'\n",
        "\n",
        "model_resnet18, opt_resnet = train_model(model_resnet18, train_batch_gen, val_batch_gen, opt_resnet,\n",
        "                                         ckpt_name=ckpt_name_resnet18, n_epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pErJSWrXQAXJ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OI6M_HctQAXL",
        "colab": {}
      },
      "source": [
        "best_model_resnet18=None\n",
        "with open(ckpt_name_resnet18, 'rb') as f:\n",
        "    best_model_resnet18 = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model_resnet18, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model_resnet18, test_batch_gen, 'test')\n",
        "\n",
        "if val_stats['f1_score'] > 0.85 and test_stats['f1_score'] > 0.85:\n",
        "    print('You have achieved the baseline for this task.')\n",
        "else:\n",
        "    print('Train for some more time.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICmzRqg5tgOz",
        "colab_type": "text"
      },
      "source": [
        "## Use your own image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGStfaR3tgO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "src_1_fp = r\"img/example_1.png\"\n",
        "src_2_fp = r\"img/example_2.png\"\n",
        "\n",
        "src_1 = imread(src_1_fp)\n",
        "src_2 = imread(src_2_fp)\n",
        "\n",
        "resized_1 = resize(src_1, (SIZE_H, SIZE_W), mode='reflect')\n",
        "resized_2 = resize(src_2, (SIZE_H, SIZE_W), mode='reflect')\n",
        "\n",
        "# convert to torch.Tensor\n",
        "tensor_1 = torch.Tensor(np.transpose((resized_1/255 - image_mean) / image_std, [2,0,1])[np.newaxis,:,:,:]).to(device)\n",
        "tensor_2 = torch.Tensor(np.transpose((resized_2/255 - image_mean) / image_std, [2,0,1])[np.newaxis,:,:,:]).to(device)\n",
        "\n",
        "# 'cat' scores\n",
        "score_1 = F.softmax(best_model_resnet18(tensor_1), 1)[0][0].detach().cpu().numpy()\n",
        "score_2 = F.softmax(best_model_resnet18(tensor_2), 1)[0][0].detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GCN-XdBtgPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_label = lambda x: ('cat' if x > 0.5 else 'dog') + ': {:.4f}'.format(x)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(121)\n",
        "plt.imshow(src_1)\n",
        "plt.title(get_label(score_1))\n",
        "plt.subplot(122)\n",
        "plt.imshow(src_2)\n",
        "plt.title(get_label(score_2))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2a7DkWvoPy_l"
      },
      "source": [
        "```\n",
        "```\n",
        "\n",
        "\n",
        "# Task 3: adding normalization and different model initialization\n",
        "## Let's get back to hard work\n",
        "* Improve the task 1 CNN architecture  with the following:\n",
        "  * Add batch norm (with default params) between convolution and ReLU\n",
        "    * nn.BatchNorm*d (1d for dense, 2d for conv)\n",
        "    * usually better to put them after linear/conv but before nonlinearity\n",
        "* Re-train the network with the same optimizer, it should get at least __80%__ validation accuracy at peak.\n",
        "* Use the following model class to simplify the inference\n",
        "\n",
        "\n",
        "To know more about **batch_norm** and **data covariate shift**\n",
        "\n",
        "https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
        "\n",
        "https://www.youtube.com/watch?v=nUUqwaxLnWs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QiEE5fUyVXe3",
        "colab": {}
      },
      "source": [
        "# Custom model class\n",
        "\n",
        "def conv_block_3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Sequential(\n",
        "        # YOUR CODE HERE\n",
        "        # CONV 3x3 -> BN -> ReLU\n",
        "        # YOUR CODE ENDS HERE\n",
        "    )\n",
        "\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self, in_feature):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            # YOUR CODE HERE: CONV_BLOCKS -> GLOBAL_POOLING (MAX/AVERAGE)\n",
        "            nn.AdaptiveMaxPool2d(1),\n",
        "            Flatten()\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Sequential(\n",
        "            # YOUR CODE HERE: FC->BN->RELU\n",
        "        )\n",
        "        self.pred = nn.Sequential(\n",
        "            nn.Linear(EMBEDDING_SIZE, NUM_CLASSES, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.pred(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JoiUENLaERSO",
        "colab": {}
      },
      "source": [
        "# outputs are here for convenience\n",
        "model_cnn_norm = MyModel(3)\n",
        "summary(model_cnn_norm, (3, SIZE_H, SIZE_W), device='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3B_3KKCzGdDn"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aHj7WEFYGfPO",
        "colab": {}
      },
      "source": [
        "model_cnn_norm = model_cnn_norm.to(device)\n",
        "opt = torch.optim.Adam(model_cnn_norm.parameters(), lr=1e-3)\n",
        "ckpt_name_cnn_norm='model_cnn_norm.ckpt'\n",
        "model_cnn_norm, opt = train_model(model_cnn_norm, train_batch_gen, val_batch_gen, opt, ckpt_name=ckpt_name_cnn_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9v59vlpQAWh"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJhw1ZFAI5VW",
        "colab": {}
      },
      "source": [
        "best_model_cnn_norm = None\n",
        "with open(ckpt_name_cnn_norm, 'rb') as f:\n",
        "    best_model_cnn_norm = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model_cnn_norm, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model_cnn_norm, test_batch_gen, 'test')\n",
        "\n",
        "if val_stats['f1_score'] > 0.8 and test_stats['f1_score'] > 0.8:\n",
        "    print('You have achieved the baseline for this task.')\n",
        "else:\n",
        "    print('Train for some more time or change CNN architecture.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yUaWZ-Q3no7z"
      },
      "source": [
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "# Task 4: Data Augmentation (bonus area)\n",
        "\n",
        "There's a powerful torch tool for image preprocessing useful to do data preprocessing and augmentation.\n",
        "\n",
        "Here's how it works: we define a pipeline that\n",
        "* makes random crops of data (augmentation)\n",
        "* randomly changes image color (augmentation)\n",
        "* randomly flips image horizontally (augmentation)\n",
        "* then normalizes it (preprocessing)\n",
        "\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/augmentation.png?raw=1\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P8pjyFNMPAO1",
        "colab": {}
      },
      "source": [
        "transformer_augmented = transforms.Compose([\n",
        "    # YOUR CODE HERE\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(image_mean, image_std)]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NXFq2IEAoTmU",
        "colab": {}
      },
      "source": [
        "# Load dataset using ImageFolder using transformer with augmentation\n",
        "# Note: We do not use augmentation for validation or testing\n",
        "train_dataset_aug = # YOUR CODE HERE: creade dataset using the transformer above\n",
        "train_aug_batch_gen = torch.utils.data.DataLoader(train_dataset_aug, \n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=NUM_WORKERS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eLnYAStWHSg1"
      },
      "source": [
        "Let's look at some image examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1XIMfpuFHOVI",
        "colab": {}
      },
      "source": [
        "plot_from_batch_generator(train_aug_batch_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bebUYiLuqXpG"
      },
      "source": [
        "Note that we did not change test_dataset, as we do not need to augment image data in it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0mUZaS_nqzhY"
      },
      "source": [
        "Let's retrain our model, saving it to another variable\n",
        "\n",
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sj_ORF1mH4jc",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "model_cnn_aug = MyModel(3).to(device)\n",
        "opt = torch.optim.Adam(model_cnn_aug.parameters(), lr=1e-3)\n",
        "ckpt_name_aug='model_cnn_aug.ckpt'\n",
        "model_cnn_aug, opt = train_model(model_cnn_aug, train_aug_batch_gen, val_batch_gen, opt,\n",
        "                                                         ckpt_name=ckpt_name_aug, n_epochs=2 * EPOCH_NUM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfnOnp_e9PTD",
        "colab": {}
      },
      "source": [
        "## Evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Gj-eijtgQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_model_cnn_aug=None\n",
        "with open(ckpt_name_aug, 'rb') as f:\n",
        "    best_model_cnn_aug = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model_cnn_aug, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model_cnn_aug, test_batch_gen, 'test')\n",
        "if val_stats['f1_score'] > 0.9 and test_stats['f1_score'] > 0.9:\n",
        "    print('You have achieved the baseline for this task.')\n",
        "else:\n",
        "    print('Train for some more time or change augmentation scheme.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iau2ysbMQAW8",
        "colab": {}
      },
      "source": [
        "best_model_cnn_aug=None\n",
        "with open(ckpt_name_aug, 'rb') as f:\n",
        "    best_model_cnn_aug = torch.load(f)\n",
        "\n",
        "val_stats = test_model(best_model_cnn_aug, val_batch_gen, 'val')\n",
        "test_stats = test_model(best_model_cnn_aug, test_batch_gen, 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-oIIaW_tgRN",
        "colab_type": "text"
      },
      "source": [
        "## Dark Magic (some practical tricks and issues)\n",
        "\n",
        "<img src=\"https://github.com/neychev/harbour_ml2020/blob/master/day12_Convolutional_Neural_Networks/img/dm.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLrBwK_mtgRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}