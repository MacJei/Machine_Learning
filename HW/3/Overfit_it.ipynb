{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Overfit it.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
      "language": "python",
      "name": "python37464bitanaconda3virtualenv32645142025941c89ba0a8870ffeabe0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "NFmOh482SyEF"
      },
      "outputs": [],
      "source": [
        "## Assignment 3: Dealing with overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "AjzAuO3oSvsI"
      },
      "outputs": [],
      "source": [
        "Today we work with [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) (*hint: it is available in `torchvision`*).\n",
        "\n",
        "Your goal for today:\n",
        "1. Train a FC (fully-connected) network that achieves >= 0.885 test accuracy.\n",
        "2. Cause considerable overfitting by modifying the network (e.g. increasing the number of network parameters and/or layers) and demonstrate in in the appropriate way (e.g. plot loss and accurasy on train and validation set w.r.t. network complexity).\n",
        "3. Try to deal with overfitting (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results.\n",
        "\n",
        "__Please, write a small report describing your ideas, tries and achieved results in the end of this file.__\n",
        "\n",
        "*Note*: Tasks 2 and 3 are interrelated, in task 3 your goal is to make the network from task 2 less prone to overfitting. Task 1 is independent from 2 and 3.\n",
        "\n",
        "*Note 2*: We recomment to use Google Colab or other machine with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchsummary\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Technical function\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(root_path):\n",
        "        os.mkdir(root_path)\n",
        "        print('Directory', path, 'is created!')\n",
        "    else:\n",
        "        print('Directory', path, 'already exists!')\n",
        "        \n",
        "root_path = 'fmnist'\n",
        "mkdir(root_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "download = True\n",
        "train_transform = transforms.ToTensor()\n",
        "test_transform = transforms.ToTensor()\n",
        "transforms.Compose((transforms.ToTensor()))\n",
        "\n",
        "\n",
        "fmnist_dataset_train = torchvision.datasets.FashionMNIST(root_path, \n",
        "                                                        train=True, \n",
        "                                                        transform=train_transform,\n",
        "                                                        target_transform=None,\n",
        "                                                        download=download)\n",
        "fmnist_dataset_test = torchvision.datasets.FashionMNIST(root_path, \n",
        "                                                       train=False, \n",
        "                                                       transform=test_transform,\n",
        "                                                       target_transform=None,\n",
        "                                                       download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_size = int(.8 * len(fmnist_dataset_train.data))\n",
        "validation_data_size = len(fmnist_dataset_train) - train_data_size\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "train_dataset, validate_dataset = random_split(\n",
        "                                                 fmnist_dataset_train,\n",
        "                                                 [train_data_size, validation_data_size]\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(fmnist_dataset_train, \n",
        "                                           batch_size=128,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(fmnist_dataset_test,\n",
        "                                          batch_size=256,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(fmnist_dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for img, label in train_loader:\n",
        "    # print('img:',img)\n",
        "    print(img.shape)\n",
        "    # print('label',label)\n",
        "    print(label.shape)\n",
        "    print(type(label))\n",
        "    print(label.size(0))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "b6OOOffHTfX5"
      },
      "outputs": [],
      "source": [
        "### Task 1\n",
        "Train a network that achieves $\\geq 0.885$ test accuracy. It's fine to use only Linear (`nn.Linear`) layers and activations/dropout/batchnorm. Convolutional layers might be a great use, but we will meet them a bit later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # Your network structure comes here\n",
        "            nn.Linear(input_shape,400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p=.23,inplace= False),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, inp):       \n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torchsummary.summary(TinyNeuralNetwork(), (28*28,))"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "544PGKEnjPr5"
      },
      "outputs": [],
      "source": [
        "Your experiments come here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_train_loader = torch.utils.data.DataLoader( train_dataset, \n",
        "                                                batch_size=128,\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=2\n",
        "                                              )\n",
        "new_validation_loader = torch.utils.data.DataLoader( validate_dataset, \n",
        "                                                     batch_size=128,\n",
        "                                                     shuffle=True,\n",
        "                                                     num_workers=2\n",
        "                                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Batch 1 of 40 took 19.462s\n\t  training loss: 1.262543\n\tvalidation loss: 1.101728\n\tvalidation accuracy: 0.765\nBatch 2 of 40 took 19.662s\n\t  training loss: 1.053084\n\tvalidation loss: 1.023764\n\tvalidation accuracy: 0.836\nBatch 3 of 40 took 19.614s\n\t  training loss: 1.012431\n\tvalidation loss: 0.995610\n\tvalidation accuracy: 0.857\nBatch 4 of 40 took 19.645s\n\t  training loss: 0.993548\n\tvalidation loss: 0.997557\n\tvalidation accuracy: 0.855\nBatch 5 of 40 took 18.906s\n\t  training loss: 0.981815\n\tvalidation loss: 0.990174\n\tvalidation accuracy: 0.858\nBatch 6 of 40 took 20.779s\n\t  training loss: 0.974015\n\tvalidation loss: 0.975938\n\tvalidation accuracy: 0.870\nBatch 7 of 40 took 23.943s\n\t  training loss: 0.964891\n\tvalidation loss: 0.967490\n\tvalidation accuracy: 0.872\nBatch 8 of 40 took 24.363s\n\t  training loss: 0.958195\n\tvalidation loss: 0.968061\n\tvalidation accuracy: 0.872\nBatch 9 of 40 took 16.517s\n\t  training loss: 0.955048\n\tvalidation loss: 0.964873\n\tvalidation accuracy: 0.877\nBatch 10 of 40 took 17.993s\n\t  training loss: 0.949063\n\tvalidation loss: 0.960593\n\tvalidation accuracy: 0.880\nBatch 11 of 40 took 17.275s\n\t  training loss: 0.946623\n\tvalidation loss: 0.964766\n\tvalidation accuracy: 0.880\nBatch 12 of 40 took 20.376s\n\t  training loss: 0.940955\n\tvalidation loss: 0.963255\n\tvalidation accuracy: 0.877\nBatch 13 of 40 took 22.200s\n\t  training loss: 0.937374\n\tvalidation loss: 0.955614\n\tvalidation accuracy: 0.886\nBatch 14 of 40 took 22.217s\n\t  training loss: 0.935085\n\tvalidation loss: 0.968332\n\tvalidation accuracy: 0.877\nBatch 15 of 40 took 17.362s\n\t  training loss: 0.933808\n\tvalidation loss: 0.959815\n\tvalidation accuracy: 0.885\nBatch 16 of 40 took 19.080s\n\t  training loss: 0.928870\n\tvalidation loss: 0.953606\n\tvalidation accuracy: 0.887\nBatch 17 of 40 took 18.268s\n\t  training loss: 0.925454\n\tvalidation loss: 0.959899\n\tvalidation accuracy: 0.885\nBatch 18 of 40 took 14.759s\n\t  training loss: 0.923387\n\tvalidation loss: 0.952758\n\tvalidation accuracy: 0.886\nBatch 19 of 40 took 17.596s\n\t  training loss: 0.919699\n\tvalidation loss: 0.952075\n\tvalidation accuracy: 0.889\nBatch 20 of 40 took 18.950s\n\t  training loss: 0.918991\n\tvalidation loss: 0.954718\n\tvalidation accuracy: 0.891\nBatch 21 of 40 took 19.895s\n\t  training loss: 0.916058\n\tvalidation loss: 0.953364\n\tvalidation accuracy: 0.888\nBatch 22 of 40 took 31.619s\n\t  training loss: 0.914454\n\tvalidation loss: 0.953027\n\tvalidation accuracy: 0.889\nBatch 23 of 40 took 27.479s\n\t  training loss: 0.913272\n\tvalidation loss: 0.955406\n\tvalidation accuracy: 0.891\nBatch 24 of 40 took 24.791s\n\t  training loss: 0.910792\n\tvalidation loss: 0.953456\n\tvalidation accuracy: 0.889\nBatch 25 of 40 took 37.827s\n\t  training loss: 0.911482\n\tvalidation loss: 0.952694\n\tvalidation accuracy: 0.889\nBatch 26 of 40 took 20.869s\n\t  training loss: 0.908607\n\tvalidation loss: 0.952000\n\tvalidation accuracy: 0.890\nBatch 27 of 40 took 25.435s\n\t  training loss: 0.905400\n\tvalidation loss: 0.972620\n\tvalidation accuracy: 0.883\nBatch 28 of 40 took 25.151s\n\t  training loss: 0.906938\n\tvalidation loss: 0.951381\n\tvalidation accuracy: 0.894\n"
        }
      ],
      "source": [
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss, NLLLoss\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "import time\n",
        "\n",
        "model = TinyNeuralNetwork().to(dtype=torch.float32)\n",
        "optimizer = optim.AdamW(model.parameters(), lr = 3e-4)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# loss_function = NLLLoss()\n",
        "\n",
        "                    # *** Train experiment ***\n",
        "\n",
        "def exp1_train_the_model(model, train_loader, validation_loader, loss_function, batches = int):\n",
        "    train_loss = []\n",
        "    validation_loss = []\n",
        "    validation_accuracy = []\n",
        "\n",
        "    for b in range(batches):\n",
        "        b_train_loss = []\n",
        "        b_validation_loss = []\n",
        "        b_validation_accuracy = []\n",
        "        timer = time.time()\n",
        "\n",
        "        model.train(True)\n",
        "        for x, y in train_loader:\n",
        "            training_loss = loss_function(\n",
        "                                            model(x), \n",
        "                                            y\n",
        "                                         )\n",
        "            training_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            b_train_loss.append(training_loss.item())\n",
        "        \n",
        "        model.train(False)\n",
        "        with torch.no_grad():\n",
        "            for x, y in validation_loader:\n",
        "                test_prediction = model(x)\n",
        "                testing_loss = loss_function(\n",
        "                                            test_prediction, \n",
        "                                            y\n",
        "                                         )\n",
        "                b_validation_loss.append(testing_loss)\n",
        "                test_prediction_max = test_prediction.max(1)[1].data\n",
        "                b_validation_accuracy.append((test_prediction_max == y).to(torch.float32).mean().item())\n",
        "        print(f'Batch {b + 1} of {batches} took {time.time() - timer:.3f}s')\n",
        "\n",
        "        train_loss.append(np.mean(b_train_loss))\n",
        "        validation_loss.append(np.mean(b_validation_loss))\n",
        "        validation_accuracy.append(np.mean(b_validation_accuracy))\n",
        "        \n",
        "        print(f\"\\t  training loss: {train_loss[-1]:.6f}\")\n",
        "        print(f\"\\tvalidation loss: {validation_loss[-1]:.6f}\")\n",
        "        print(f\"\\tvalidation accuracy: {validation_accuracy[-1]:.3f}\")\n",
        "\n",
        "    return train_loss, validation_loss, validation_accuracy\n",
        "\n",
        "                    # *** Plot results of Experiment ***\n",
        "def plot_exp1(train_loss, validation_loss, validation_accuracy):\n",
        "    fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
        "\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].plot(train_loss, label='Train Loss')\n",
        "    axes[0].plot(validation_loss, label='Validation Loss')\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].set_title('Validation Accuracy')\n",
        "    axes[1].plot(validation_accuracy)\n",
        "    axes[1].legend()\n",
        "\n",
        "batch_size = 40\n",
        "train_loss, validation_loss, validation_accuracy = exp1_train_the_model(\n",
        "                                                                        model,\n",
        "                                                                        new_train_loader,\n",
        "                                                                        new_validation_loader,\n",
        "                                                                        loss_function,\n",
        "                                                                        batch_size             \n",
        "                                                                        ) \n",
        "                                                            \n",
        "# Your experiments, training and validation loops here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_exp1(train_loss, validation_loss, validation_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "L7ISqkjmCPB1"
      },
      "outputs": [],
      "source": [
        "### Task 2: Overfit it.\n",
        "Build a network that will overfit to this dataset. Demonstrate the overfitting in the appropriate way (e.g. plot loss and accurasy on train and test set w.r.t. network complexity).\n",
        "\n",
        "*Note:* you also might decrease the size of `train` dataset to enforce the overfitting and speed up the computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OverfittingNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), # This layer converts image into a vector to use Linear layers afterwards\n",
        "            # Your network structure comes here\n",
        "            nn.Linear(input_shape, 2000),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(2000, 1000),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(500, 300),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, inp):   \n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torchsummary.summary(OverfittingNeuralNetwork().to(device), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "overfit_model = OverfittingNeuralNetwork().to(device)\n",
        "overfit_optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)    # YOUR CODE HERE\n",
        "overfit_loss_function = NLLLoss()                                       # YOUR CODE HERE\n",
        "\n",
        "# Your experiments, come here\n",
        "batch_size = 80\n",
        "overfit_train_loss, overfit_validation_loss, overfit_validation_accuracy=exp1_train_the_model(\n",
        "                                                                    overfit_model,\n",
        "                                                                    new_train_loader,\n",
        "                                                                    new_validation_loader,\n",
        "                                                                    overfit_loss_function,\n",
        "                                                                    batch_size\n",
        "                                                                       )\n",
        "plot_exp1(overfit_train_loss, overfit_validation_loss, overfit_validation_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "podB-m_aBZXQ"
      },
      "outputs": [],
      "source": [
        "### Task 3: Fix it.\n",
        "Fix the overfitted network from the previous step (at least partially) by using regularization techniques (Dropout/Batchnorm/...) and demonstrate the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FixedNeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "            *** How to fix the pb of overfitting ***\n",
        "            === === === === === === === === === === ===\n",
        "        >>> Use less layers\n",
        "        >>> Use a better optimizer\n",
        "        >>> Use DropOut, BatchNorm\n",
        "        >>> Predict on Data the model has never seen --> Cross_validation \n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape=28*28, num_classes=10, input_channels=1):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(input_shape, 2000),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p = .35, inplace= False),\n",
        "            nn.Linear(2000, 1000),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1000, 500),\n",
        "            nn.BatchNorm1d(500, eps=1e-5),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(p=.23,inplace=False),\n",
        "            nn.Linear(500, 300),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(300, 100),\n",
        "            nn.Tanh(),\n",
        "            nn.BatchNorm1d(100, eps=1e-5),\n",
        "            nn.Linear(100, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, inp):       \n",
        "        out = self.model(inp)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torchsummary.summary(FixedNeuralNetwork().to(device), (28*28,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fixed_model = FixedNeuralNetwork().to(device)\n",
        "fixed_optimizer = optim.AdamW(fixed_model.parameters(),lr=3e-4)# YOUR CODE HERE\n",
        "fixed_loss_function = nn.CrossEntropyLoss() # YOUR CODE HERE\n",
        "\n",
        "# Your experiments, come here\n",
        "batch_size = 40\n",
        "fixed_train_loss_1, fixed_validation_loss_1, fixed_validation_accuracy_1 = exp1_train_the_model(\n",
        "                                                                                                    fixed_model,\n",
        "                                                                                                    train_loader,\n",
        "                                                                                                    test_loader,\n",
        "                                                                                                    fixed_loss_function,\n",
        "                                                                                                    batch_size\n",
        "                                                                                                )\n",
        "fixed_train_loss_2, fixed_validation_loss_2, fixed_validation_accuracy_2 = exp1_train_the_model(\n",
        "                                                                                                    fixed_model,\n",
        "                                                                                                    new_train_loader,\n",
        "                                                                                                    new_validation_loader,\n",
        "                                                                                                    fixed_loss_function,\n",
        "                                                                                                    batch_size\n",
        "                                                                                                )\n",
        "plot_exp1(overfit_train_loss, overfit_validation_loss, overfit_validation_accuracy)\n",
        "plot_exp1(fixed_train_loss_1, fixed_validation_loss_1, fixed_validation_accuracy_1)\n",
        "plot_exp1(fixed_train_loss_2, fixed_validation_loss_2, fixed_validation_accuracy_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "dMui_uLJ7G0d"
      },
      "outputs": [],
      "source": [
        "### Conclusions:\n",
        "_Write down small report with your conclusions and your ideas._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"  \n",
        "    In conclusion, Here is what i've learnt from this dataset and about this intro to Deep learning;\n",
        "    >>> Using one's computing power with CPU means the PC is going to spend alot more time than others using GPU.\n",
        "    >>> We can use a dataloader the same way as we use the basic dataset. Just call some functions on the dataset.\n",
        "    >>> It is easy to overfit a dataset without knowing, either by --> Training the model for too long\n",
        "                                                                   --> Using too many hidden layers. It's wise to know that simple is usually best\n",
        "                                                                   --> Generating too many unnnecessary features within the many layers\n",
        "                                                                   --> Leaking data by using data the model has seen before\n",
        "    >>> To avoid the possibility of overfitting we can --> Split the data\n",
        "                                                       --> normalize the data every other time\n",
        "                                                       --> use a good optimizer\n",
        "                                                       --> Use less layers an uncomplicated model\n",
        "                                                       --> During the training we should zero out some neurons of our network to reduce corelation\n",
        "                                                       --> Do cross validation or in the least predict on data the model hasn't seen before\n",
        "                                                       --> Do fewer iterations during training so the model doesn't have enough time to memorize the data.\n",
        "\"\"\""
      ]
    }
  ]
}